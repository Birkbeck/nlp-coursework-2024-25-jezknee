Answers to the text questions go here.

1(d) When is the Flesch Kincaid score *not* a valid, robust or reliable estimator of text difficulty? Give two conditions. (Text answer, 200 words maximum).

The score gives a very general answer to text difficulty, without knowledge of the reader’s own background. In circumstances where the reader has a high level of technical understanding, a difficult text may actually be far easier to comprehend.
The metric also doesn’t take into account how well-formed the sentences are, just how long they are and how long the words are. Vague or misleading phrasing would increase the text’s difficulty, but wouldn’t necessarily be captured by the metric. Poor syntax or grammar would also be missed.
It also doesn’t measure how common a word is: a short, unusual word like “abb” or  “fa” would lower the Flesch-Kincaid score, but would reduce readability. Similarly, abbreviations would decrease the score, but the reader might not understand them.
In languages where words can be combined together to form new words, it might be easy to create simple words that are very long. For example, “Haustürschlüssel” (“front door key”) increases the document's Flesch-Kincaid score, but is easy to understand if you know the simple component parts.

2(f) Explain your tokenizer function and discuss its performance

Tokeniser explanation:
In my custom tokeniser, I used the spacy tokeniser to pull out each document’s nouns, proper nouns, adjectives, adverbs and verbs. I also used RegEx in a custom preprocessing function to remove everything in round or square brackets from the text, as these are editorial annotations, rather than text from the speeches themselves. I then performed feature selection using SelectKBest and a chi squared metric to only select the k tokens with the highest correlation to the Party label.
The highest performance is the SVM classifier, which reports an accuracy of 0.82, compared to 0.81 for the standard tokeniser’s SVM classifier, which isn’t a significant improvement. However, my model only has half the features of the standard one: after feature selection, it only uses 1500 features rather than 3000, making it far more efficient.

Discussion:
For this task, I attempted to apply some domain knowledge. I hypothesised that the classifier would work best if it just used nouns: when I read the speeches myself I could often identify the party from the nouns used, e.g. if the speech referred to Scotland, then the speaker was generally from the SNP, and the same logic could be applied to constituencies or UK regions. Another hypothesis was to use adjectives, verbs and adverbs in the classifier. You would expect the governing party (Conservative) to give far more positive speeches than the opposition, who you would expect to be more critical. These stances should reveal themselves in the choice of adjectives and adverbs, e.g. the Conservative party would use certain adjectives such as “brilliant” or “great” more frequently. 
However, the classifier performed better if I gave it more words rather than fewer, then performed feature selection using a chi squared test to select the best k tokens – accuracy improved when I let the machine learn for itself, rather than applying too much of my own domain knowledge. In all, I tried many, many combinations of token.pos_ to improve performance (e.g. just nouns, just nouns and proper nouns, everything except nouns, etc.), but it always performed worse than the standard tokeniser. This suggests that, at least in this case, applying a custom tokeniser on its own is unlikely to improve accuracy much. However, customising the tokeniser allowed me to limit further what was input into the model, and so allowed me to reduce the number of features, particularly when I combined my tokeniser with preprocessing and feature selection.
There is one way in which accuracy significantly improved. In general, the classifiers perform worst for the Liberal Democrat party, which makes sense. The Lib Dems had few MPs at the time and so would have given fewer speeches, plus as an opposition party they were likely to be critical in a similar way to Labour and the SNP. In this respect my tokeniser does much better than the standard one, increasing accuracy by 0.11.
The custom tokeniser takes far longer to run than the standard tokeniser (up to 10 minutes longer), and only has marginally better performance, i.e. 0.82 rather than 0.81. However, it does allow me to reduce the number of features significantly in feature selection (1500 features rather than 3000) without diminishing accuracy (SVM accuracy was 0.82 for both models; RandomForest was slightly more accurate for the smaller custom tokeniser model than the larger standard model). The standard model also becomes more accurate if you increase the size of the max_features parameter in the tokeniser itself, which doesn’t happen with my custom tokeniser. Given that my custom tokeniser can slightly improve on the standard tokeniser’s performance with only half the features, I would expect my tokeniser to generalise much better to a larger corpus of Hansard speeches.

